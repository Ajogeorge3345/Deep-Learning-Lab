import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from keras.layers import Input, LSTM, Embedding, Dense
from keras.models import Model

# Load and preprocess the dataset
data = pd.read_csv('dataset.csv', encoding='utf-8')
all_eng_words = set()
all_hin_words = set()

for eng, hin in zip(data['English'], data['Hindi']):
    all_eng_words.update(eng.split())
    all_hin_words.update(hin.split())

data['len_eng_sen'] = data['English'].apply(lambda x: len(x.split()))
data['len_hin_sen'] = data['Hindi'].apply(lambda x: len(x.split()))
data = data[(data['len_eng_sen'] <= 20) & (data['len_hin_sen'] <= 20)]

max_len_src = max(data['len_hin_sen'])
max_len_tar = max(data['len_eng_sen'])
inp_words = sorted(list(all_eng_words))
tar_words = sorted(list(all_hin_words))
num_enc_toks = len(all_eng_words)
num_dec_toks = len(all_hin_words) + 1 # plus 1 for zero padding

inp_tok_idx = {word: i + 1 for i, word in enumerate(inp_words)}
tar_tok_idx = {word: i + 1 for i, word in enumerate(tar_words)}
rev_inp_char_idx = {i: word for word, i in inp_tok_idx.items()}
rev_tar_char_idx = {i: word for word, i in tar_tok_idx.items()}

X, y = data['English'], data['Hindi']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
batch_size = 128

# Generator function with explicit shape and dtype for TensorFlow Dataset
def generate_batch_data(X, y, batch_size):
    while True:
        enc_inp_data = np.zeros((batch_size, max_len_src), dtype='float32')
        dec_inp_data = np.zeros((batch_size, max_len_tar), dtype='float32')
        dec_tar_data = np.zeros((batch_size, max_len_tar, num_dec_toks), dtype='float32')

        for i, (inp_text, tar_text) in enumerate(zip(X, y)):
            if i >= batch_size:
                break
            for t, word in enumerate(inp_text.split()):
                enc_inp_data[i, t] = inp_tok_idx.get(word, 0)
            for t, word in enumerate(tar_text.split()):
                if t < len(tar_text.split()) - 1:
                    dec_inp_data[i, t] = tar_tok_idx.get(word, 0)
                if t > 0:
                    dec_tar_data[i, t - 1, tar_tok_idx.get(word, 0)] = 1.0
        yield (enc_inp_data, dec_inp_data), dec_tar_data

# Use tf.data.Dataset.from_generator with explicit output signature
def create_dataset(X, y, batch_size):
    return tf.data.Dataset.from_generator(
        lambda: generate_batch_data(X, y, batch_size),
        output_signature=(
            (
                tf.TensorSpec(shape=(batch_size, max_len_src), dtype=tf.float32),
                tf.TensorSpec(shape=(batch_size, max_len_tar), dtype=tf.float32),
            ),
            tf.TensorSpec(shape=(batch_size, max_len_tar, num_dec_toks), dtype=tf.float32),
        )
    )

# Create train and validation datasets
train_dataset = create_dataset(X_train, y_train, batch_size).repeat()
val_dataset = create_dataset(X_test, y_test, batch_size).repeat()

# Model architecture
latent_dim = 250
enc_inps = Input(shape=(None,))
enc_emb = Embedding(num_enc_toks, latent_dim, mask_zero=True)(enc_inps)
enc_lstm = LSTM(latent_dim, return_state=True)
enc_outputs, st_h, st_c = enc_lstm(enc_emb)
enc_states = [st_h, st_c]

# Decoder setup
dec_inps = Input(shape=(None,))
dec_emb_layer = Embedding(num_dec_toks, latent_dim, mask_zero=True)
dec_emb = dec_emb_layer(dec_inps)
dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
dec_outputs, _, _ = dec_lstm(dec_emb, initial_state=enc_states)
dec_dense = Dense(num_dec_toks, activation='softmax')
dec_outputs = dec_dense(dec_outputs)

# Define and compile the model
model = Model([enc_inps, dec_inps], dec_outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy')

# Train the model
train_samples = len(X_train)
val_samples = len(X_test)
model.fit(
    train_dataset,
    steps_per_epoch=train_samples // batch_size,
    epochs=20,
    validation_data=val_dataset,
    validation_steps=val_samples // batch_size
)

# Define encoder model for inference
enc_model = Model(enc_inps, enc_states)

# Define decoder model for inference
dec_st_inp_h = Input(shape=(latent_dim,))
dec_st_inp_c = Input(shape=(latent_dim,))
dec_states_inps = [dec_st_inp_h, dec_st_inp_c]
dec_emb2 = dec_emb_layer(dec_inps)
dec_outputs2, st_h2, st_c2 = dec_lstm(dec_emb2, initial_state=dec_states_inps)
dec_states2 = [st_h2, st_c2]
dec_outputs2 = dec_dense(dec_outputs2)
dec_model = Model([dec_inps] + dec_states_inps, [dec_outputs2] + dec_states2)

# Function to translate a given English sentence
def translate_sentence(sentence):
    input_seq = []
    for word in sentence.split():
        input_seq.append(inp_tok_idx.get(word, 0))

    input_seq = np.array([input_seq])

    states_value = enc_model.predict(input_seq)
    tar_seq = np.zeros((1, 1))
    tar_seq[0, 0] = tar_tok_idx['START_']
    stop_cond = False
    dec_sen = ''

    while not stop_cond:
        output_toks, h, c = dec_model.predict([tar_seq] + states_value)
        sampled_tok_idx = np.argmax(output_toks[0, -1, :])
        sampled_char = rev_tar_char_idx[sampled_tok_idx]
        dec_sen += ' ' + sampled_char

        if sampled_char == '_END' or len(dec_sen) > 50:
            stop_cond = True

        tar_seq = np.zeros((1, 1))
        tar_seq[0, 0] = sampled_tok_idx
        states_value = [h, c]

    return dec_sen

# Test the model on a few sentences from the test set
for i in range(10):
    input_sentence = X_test.iloc[i]
    actual_output = y_test.iloc[i]

    predicted_output = translate_sentence(input_sentence)

    print(f"Input English sentence: {input_sentence}")
    print(f"Predicted Hindi Translation: {predicted_output}")
    print(f"Actual Hindi Translation: {actual_output}")
    print()
